# Template/example config-file for PPO

[SETUP]
log_dir = training_logs/ppo/test8
# Port for CARLA serer
port = 2000
device = cuda
num_workers = 0
batch_size = 20
# Resume training with saved actor and critic networks.
# Make sure that the paths for the actor and critic networks correspond with the 'resume_episode' number
resume_episode = 7
# Legal values: 'None' | 'gt' | 'trained'.
computer_vision = None
# Show pygame window during rollouts
show = True

[TRAINING]
max_episode = 50
max_rollout_length = 600
epoch_per_episode = 1
rollouts_per_episode = 8
# Parameter for clipping updates to the the policy objective
clip_ratio = 0.05
# Parameters for generalized advantage estimation. When lambda = 1, advantages are calculated using discounted rewards
gamma = 0.99
lambda = 0.95
# c1 is the critic criterion coefficient.
c1 = 1
# c2 is the entropy coefficient. Used to ensure sufficient exploration.
c2 = 0.001

[REWARD]
# Reward function: # alpha * speed + (-beta * distance_from_optimal_path) + (-phi * speed - delta) * I(i)
# I(i) = 1 if collision happens at time step 'i'
alpha = 1
beta = 1
phi = 250
delta = 250

[AGENT]
action_std = 0.03
min_action_std = 0.01
decay_rate = 0.0001
# How many time-steps between each decay
decay_frequency = 10000

[ACTOR]
# Leave this field blank to initialize a new actor
actor_ckpt = training_logs/ppo/test8/actor-7.th
learning_rate = 1e-4
imagenet_pretrained = True
# Legal values: 'resnet18', 'resnet34', 'resnet50'
backbone = resnet34

[CRITIC]
# Leave this field blank to initialize a new critic
critic_ckpt = training_logs/ppo/test8/critic-7.th
learning_rate = 1e-4
# Legal values: 'resnet18', 'resnet34', 'resnet50'
backbone = resnet18