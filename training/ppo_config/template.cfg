[SETUP]
log_dir = training_logs/ppo/test
port = 2000
device = cuda
num_workers = 0
batch_size = 20
# Resume training with saved and critic networks
resume = False
# Legal values: 'None' | 'gt' | 'trained'.
computer_vision = None
# Show pygame window during rollouts
show = True

[TRAINING]
max_episode = 50
max_rollout_length = 4000
rollouts_per_episode = 4
epoch_per_episode = 5
# Parameter for clipping updates to the the policy objective
clip_ratio = 0.05
# Parameters for generalized advantage estimation. When lambda = 1, advantages are calculated using discounted rewards
gamma = 0.99
lambda = 0.95
# c1 is the critic criterion coefficient.
c1 = 1
# c2 is the entropy coefficient. Used to ensure sufficient exploration.
c2 = 0.001

[REWARD]
# Reward function: # alpha * speed + (-beta * distance_from_optimal_path) + (-phi * speed - delta) * I(i)
# I(i) = 1 if collision happens at time step 'i'
alpha = 1
beta = 1
phi = 250
delta = 250

[AGENT]
action_std = 0.01
min_action_std = 0.001
decay_rate = 1e-4
# How many time-steps between each decay
decay_frequency = 10000

[ACTOR]
# Leave this field blank to initialize a new actor
actor_ckpt = training_logs/phase2/reproduce3/model-10.th
learning_rate = 1e-4
imagenet_pretrained = True
# Legal values: 'resnet18', 'resnet34', 'resnet50'
backbone = resnet34

[CRITIC]
# Leave this field blank to initialize a new critic
critic_ckpt =
learning_rate = 1e-4
# Legal values: 'resnet18', 'resnet34', 'resnet50'
backbone = resnet18