[SETUP]
log_dir = training_logs/ppo/test
port = 2000
device = cuda
# Set to True to use ground truth semantic segmentation and depth estimation
use_cv  = False
# Resume training with saved and critic networks
resume = False
num_workers = 0
batch_size = 20

[TRAINING]
max_episode = 50
max_rollout_length = 4000
rollouts_per_episode = 4
epoch_per_episode = 5

# Parameter for clipping updates to the the policy objective
clip_ratio = 0.05
# Parameters for generalized advantage estimation. When lambda = 1, advantages are calculated using discounted rewards
gamma = 0.99
lambda = 0.95

[REWARD]
# Reward function: # alpha * speed + (-beta * distance_from_optimal_path) + (-phi * speed - delta) * I(i)
# I(i) = 1 if collision happens at time step 'i'
alpha = 1
beta = 1
phi = 250
delta = 250

[AGENT]
action_std = 1e-3
min_action_std = 1e-4
decay_rate = 1e-4
# How many time-steps between each decay
decay_frequency = 10000

[ACTOR]
# Leave this field blank to initialize a new actor
actor_ckpt = training_logs/phase2/reproduce3/model-10.th
learning_rate = 1e-4
imagenet_pretrained = True
# Legal values: 'ResNet18', 'ResNet34', 'ResNet50'
backbone = ResNet34
# Set to 'True' to use trained models that produce semantic segmentation and depth maps
use_trained_cv = False

[CRITIC]
# Leave this field blank to initialize a new critic
critic_ckpt =
learning_rate = 1e-4
# Legal values: 'ResNet18', 'ResNet34', 'ResNet50'
backbone = ResNet18

