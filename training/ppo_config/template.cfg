# Template/example config-file for PPO

[SETUP]
log_dir = training_logs/ppo/test12
# Port for CARLA serer
port = 2000
device = cuda
num_workers = 0
batch_size = 30
# Resume training with saved actor and critic networks.
# Make sure that the paths for the actor and critic networks correspond with the 'resume_episode' number
resume_episode = 32
# Legal values: 'None' | 'gt' | 'trained'.
computer_vision = None
# Show pygame window during rollouts
show = True

[TRAINING]
max_episode = 1000
max_rollout_length = 1000
epoch_per_episode = 1
rollouts_per_episode = 20
# Parameter for clipping updates to the the policy objective
clip_ratio = 0.05
# Parameters for generalized advantage estimation. When lambda = 1, advantages are calculated using discounted rewards
gamma = 0.97
lambda = 0.94
# c1 is the critic criterion coefficient.
c1 = 1
# c2 is the entropy coefficient. Used to ensure sufficient exploration.
c2 = 0.001

[REWARD]
# Reward function: # alpha * speed + (-beta * distance_from_optimal_path) + (-phi * speed - delta) * I(i)
# I(i) = 1 if collision happens at time step 'i'
alpha = 1
beta = 1
phi = 250
delta = 250

[AGENT]
action_std = 0.08
min_action_std = 0.005
decay_rate = 0.001
# How many time-steps between each decay
decay_frequency = 10000

[ACTOR]
# Leave this field blank to initialize a new actor
actor_ckpt = training_logs/phase1/reproduce/model-64.th
learning_rate = 1e-4
imagenet_pretrained = True
# Legal values: 'resnet18', 'resnet34', 'resnet50'
backbone = resnet34

[CRITIC]
# Leave this field blank to initialize a new critic
critic_ckpt =
learning_rate = 1e-5
# Legal values: 'resnet18', 'resnet34', 'resnet50'
backbone = resnet18
# Include the hero vehicle in the birdview
include_hero = True